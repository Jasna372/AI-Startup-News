{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --------------------------\n",
        "# CONFIG (Hardcoded for Mock Project)\n",
        "# --------------------------\n",
        "NEWS_API_KEY = \"65879868fbfe41bca2a9cad91788fdca\"\n",
        "GEMINI_API_KEY = \"AIzaSyAN51hS0QxBxod-6bI5O_L0WNyxn0n_7Sw\"\n",
        "GEMINI_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "MAX_RETRIES = 3\n",
        "INITIAL_BACKOFF = 2.0\n",
        "\n",
        "# --------------------------\n",
        "# Utilities\n",
        "# --------------------------\n",
        "def safe_parse_json(content: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Safely parse JSON from Gemini output.\"\"\"\n",
        "    if not content:\n",
        "        return []\n",
        "\n",
        "    # Clean markdown code blocks if present\n",
        "    content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback: try to find the list brackets\n",
        "        try:\n",
        "            start = content.index(\"[\")\n",
        "            end = content.rindex(\"]\") + 1\n",
        "            return json.loads(content[start:end])\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "def make_short_article(article: Dict[str, Any]) -> Dict[str, str]:\n",
        "    \"\"\"Reduces token usage by sending only necessary text.\"\"\"\n",
        "    return {\n",
        "        \"title\": (article.get(\"title\") or \"\")[:260],\n",
        "        \"description\": (article.get(\"description\") or \"\")[:600],\n",
        "        \"source\": (article.get(\"source\") or {}).get(\"name\", \"\")\n",
        "    }\n",
        "\n",
        "# --------------------------\n",
        "# Fetch NewsAPI Articles\n",
        "# --------------------------\n",
        "def fetch_news_from_newsapi(query=\"AI startup\", page_size=50):\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"language\": \"en\",\n",
        "        \"sortBy\": \"publishedAt\",\n",
        "        \"pageSize\": page_size,\n",
        "        \"apiKey\": NEWS_API_KEY\n",
        "    }\n",
        "    try:\n",
        "        print(f\"ðŸ“¡ Fetching news for query: '{query}'...\")\n",
        "        r = requests.get(url, params=params, timeout=12)\n",
        "        if r.status_code == 429:\n",
        "            print(\"â— NewsAPI rate limit reached\")\n",
        "            return []\n",
        "        r.raise_for_status()\n",
        "        articles = r.json().get(\"articles\", [])\n",
        "        print(f\"âœ… Fetched {len(articles)} articles.\")\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(\"âŒ Error fetching NewsAPI:\", e)\n",
        "        return []\n",
        "\n",
        "# --------------------------\n",
        "# Gemini Batch Processing (The \"Analyst\")\n",
        "# --------------------------\n",
        "def gemini_process_batch(batch_articles: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
        "    attempt = 0\n",
        "    backoff = INITIAL_BACKOFF\n",
        "\n",
        "    # --- CRITICAL UPDATE: Hype Filter & Dedup Logic in Prompt ---\n",
        "    prompt = (\n",
        "        \"You are a strict Data Analyst for an Investment Firm.\\n\"\n",
        "        \"Task: Extract structured data from these articles.\\n\\n\"\n",
        "        \"RULES:\\n\"\n",
        "        \"1. HYPE FILTER: Ignore articles that are low-value, pure marketing fluff, or generic 'top 10' lists. \"\n",
        "        \"Only process articles with specific events (launches, funding, acquisitions, technical breakthroughs).\\n\"\n",
        "        \"2. DEDUPLICATION ID: Generate a unique 'event_id' (slug). If two articles talk about the same event (e.g. 'OpenAI releases Sora' vs 'Sora is here'), they MUST have the IDENTICAL 'event_id' (e.g., 'openai-sora-launch').\\n\\n\"\n",
        "        \"Output Format: Return a JSON ARRAY of objects. Keys:\\n\"\n",
        "        \"- company_name (str)\\n\"\n",
        "        \"- category (str)\\n\"\n",
        "        \"- sentiment_score (float -1.0 to 1.0)\\n\"\n",
        "        \"- is_funding_news (bool)\\n\"\n",
        "        \"- event_summary (str)\\n\"\n",
        "        \"- event_id (str, lowercase-hyphenated-slug)\\n\\n\"\n",
        "        f\"Articles Data: {json.dumps(batch_articles)}\"\n",
        "    )\n",
        "\n",
        "    while attempt < MAX_RETRIES:\n",
        "        try:\n",
        "            payload = {\n",
        "                \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                GEMINI_URL,\n",
        "                params={\"key\": GEMINI_API_KEY},\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                json=payload,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                print(f\"âš ï¸ Rate limit. Waiting {backoff}s...\")\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2\n",
        "                attempt += 1\n",
        "                continue\n",
        "\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if \"candidates\" not in data or not data[\"candidates\"]:\n",
        "                return []\n",
        "\n",
        "            content = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "            parsed = safe_parse_json(content)\n",
        "            return parsed\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Batch error: {e}\")\n",
        "            attempt += 1\n",
        "            time.sleep(1)\n",
        "\n",
        "    return []\n",
        "\n",
        "# --------------------------\n",
        "# Pipeline\n",
        "# --------------------------\n",
        "def run_pipeline():\n",
        "    raw = fetch_news_from_newsapi()\n",
        "    if not raw: return\n",
        "\n",
        "    # Prepare data for LLM (Token Efficiency)\n",
        "    compact = [make_short_article(a) for a in raw]\n",
        "\n",
        "    all_structured = []\n",
        "    print(f\"\\nðŸ§  Processing {len(compact)} articles in batches of {BATCH_SIZE}...\")\n",
        "\n",
        "    for i in range(0, len(compact), BATCH_SIZE):\n",
        "        batch = compact[i:i + BATCH_SIZE]\n",
        "        print(f\"   Batch {i // BATCH_SIZE + 1} processing...\", end=\"\\r\")\n",
        "\n",
        "        parsed = gemini_process_batch(batch)\n",
        "        if parsed:\n",
        "            all_structured.extend(parsed)\n",
        "\n",
        "        time.sleep(1.5) # Respect rate limits\n",
        "\n",
        "    print(f\"\\nâœ… Extracted {len(all_structured)} items (pre-dedup).\")\n",
        "\n",
        "    # --- CRITICAL UPDATE: Deduplication Logic ---\n",
        "    # We use the LLM-generated 'event_id' to filter duplicates.\n",
        "    seen_events = set()\n",
        "    unique_data = []\n",
        "\n",
        "    for item in all_structured:\n",
        "        event_key = item.get(\"event_id\")\n",
        "\n",
        "        # If LLM failed to give ID, fallback to summary hash\n",
        "        if not event_key:\n",
        "            event_key = hashlib.md5(item.get(\"event_summary\", \"\").encode()).hexdigest()\n",
        "\n",
        "        if event_key not in seen_events:\n",
        "            seen_events.add(event_key)\n",
        "            unique_data.append(item)\n",
        "        else:\n",
        "            # Optional: Keep the one with higher information density or length?\n",
        "            # For now, first-come-first-served is fine.\n",
        "            pass\n",
        "\n",
        "    print(f\"ðŸ“‰ After Deduplication: {len(unique_data)} unique events.\")\n",
        "    save_data(unique_data)\n",
        "\n",
        "def save_data(items):\n",
        "    # Save JSON\n",
        "    with open(\"news_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(items, f, indent=2)\n",
        "\n",
        "    # Save CSV\n",
        "    if items:\n",
        "        headers = items[0].keys()\n",
        "        with open(\"news_cleaned.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=headers)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(items)\n",
        "\n",
        "    print(\"\\nðŸ’¾ Saved to 'news_cleaned.json' and 'news_cleaned.csv'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "84TEpCtdT7rB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c190c7c-a4e9-46eb-9be8-6f21de9153e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¡ Fetching news for query: 'AI startup'...\n",
            "âœ… Fetched 48 articles.\n",
            "\n",
            "ðŸ§  Processing 48 articles in batches of 8...\n",
            "\n",
            "âœ… Extracted 32 items (pre-dedup).\n",
            "ðŸ“‰ After Deduplication: 26 unique events.\n",
            "\n",
            "ðŸ’¾ Saved to 'news_cleaned.json' and 'news_cleaned.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnC_NnlOd6mf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}